{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4a9611",
   "metadata": {},
   "source": [
    "# Multimodal RAG Learning Project\n",
    "\n",
    "This notebook implements a **Retrieval-Augmented Generation (RAG)** system that processes PDF documents containing both text and images. The system uses CLIP embeddings to create a unified vector space for text and images, enabling semantic search across multimodal content.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcde5fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.messages import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import base64\n",
    "import io\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a29f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Google API key (instead of OpenAI)\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in .env file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbbcd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded!\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "print(\"CLIP model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b4a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP\"\"\"\n",
    "    if isinstance(image_data, str):  # If path\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:  # If PIL Image\n",
    "        image = image_data\n",
    "    \n",
    "    inputs=clip_processor(images=image,return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        # Normalize embeddings to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP.\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77  # CLIP's max token length\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize embeddings\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb35d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path=\"CV_HZolfaghari_new.pdf\"\n",
    "doc=fitz.open(pdf_path)\n",
    "# Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}  # Store actual image data for LLM\n",
    "\n",
    "# Text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e146181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('CV_HZolfaghari_new.pdf')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd57f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Hossein Zolfaghari\\nAI/ML Engineer\\nParis, France\\nOpen to relocation\\n\\x83 (+33) 0753142705\\n# hossein.xolf@gmail.com\\nï LinkedIn\\n§ GitHub\\nAbout Me\\nMachine Learning Engineer with 5+ years of experience building and deploying AI solutions across cloud and edge\\ndevices. I specialize in multimodal deep learning, Generative AI, and MLOps, with a strong focus on delivering models\\nthat reliably move from prototype to production. I enjoy designing scalable pipelines, improving model performance, and'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='collaborating with teams to turn complex ideas into practical, high-impact systems. I stay aligned with the latest AI\\nadvancements while keeping solutions simple, efficient, and production-ready.\\nWork Experiences\\n• Machine Learning Engineer\\n2023 – 2025\\nSESA\\nPerpignan, France\\n– Developed a hybrid multimodal machine learning model for large time-series datasets with 96% accuracy, improving\\nforecasting performance and supporting decision-making system.'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='forecasting performance and supporting decision-making system.\\n– Implemented a YOLO-based computer vision service to monitor grape growth stages in vineyard imagery, enabling\\nmore accurate crop condition tracking and supporting decision maker software.\\n– Contributed to an intelligent PV panel system that uses AI-based decision maker model to dynamically adjust panel\\norientation based on environmental and operational conditions, resulting in improving energy capture efficiency.'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='– Designed and maintained a CI/CD pipeline using GitHub Actions and AWS ECR, enabling reproducible model\\ndeployment on edge devices (Raspberry Pi).\\n• Machine Learning Engineer\\n2022 – 2023\\nSATT AXLR\\nMontpellier, France\\n– Contributed to the design and deployment of a real-time AI service for cloud detection and motion estimation (speed\\nand direction) in sky images, which improved prediction accuracy by 3%.'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='and direction) in sky images, which improved prediction accuracy by 3%.\\n– Worked on a YOLO-based object detection pipeline to identify raindrops and other visual artifacts in fisheye camera.\\n– Built an interactive Grafana dashboard to monitor data pipelines, models outputs, and performance metrics.\\n• Machine Learning Engineer\\n2022\\nPROMES-CNRS\\nPerpignan, France\\n– Developed a high-precision solar irradiance forecasting model by applying advanced time-series techniques and'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='feature engineering. The approach improved forecast accuracy by 5% over the previous model.\\n– Created a sky-state estimation model using multi-modal environmental and imaging data, achieving over 90%\\nclassification accuracy.\\n• AI Engineer\\n2020 – 2021\\nDARVIZ\\nTehran, Iran\\n– Built AI recommendation system serving 150K+ users with personalized content delivery.\\n– Developed NLP pipeline for document processing handling 10K+ documents daily.\\n• Data Analyst\\n2018 – 2020\\nIUT\\nIsfahan, Iran'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='• Data Analyst\\n2018 – 2020\\nIUT\\nIsfahan, Iran\\n– Applied stochastic modeling and uncertainty quantification to build predictive models, improving accuracy by 25%.\\n– Conducted comprehensive sensitivity and sampling analyses (Sobol indices, LHS) to evaluate model robustness and\\nparameter impact, contributing to two publications, [1, 2].\\nEducation\\n• University of Paris-Saclay | M.Sc. in Computer Vision and Artificial Intelligence\\nParis, France | 2021 - 2022'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='Paris, France | 2021 - 2022\\nThesis: Benchmarking Vision Transformers and CNNs for Multi-Modal Intra-Hour Solar Irradiance Forecasting.\\n• Isfahan University of Technology | M.Sc. in Engineering\\nIsfahan, Iran | 2014 - 2017\\n• K.N.Toosi University of Technology | B.Sc. in Engineering\\nTehran, Iran | 2009 - 2013'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='Techincal Skills\\n• Generative AI & LLMs: Transformers, Multimodal\\nAI, Agentic AI, LangChain, RAG, CrewAI, LangGraph,\\nHuggingFace, Amazon Bedrock, Prompt & Context En-\\ngineering, Fine-Tuning\\n• ML\\nTools:\\nPyTorch,\\nTensorFlow,\\nScikit-learn,\\nOpenCV, CNN, LSTM, YOLO, Feature Engineering,\\nModel Optimization\\n• MLOps & Deployment: CI/CD (GitHub Actions),\\nDocker, AWS (EC2, S3, ECR), GCP, MLflow, FastAPI,\\nFlask, Git, Monitoring (Grafana, Zabbix), Edge Deploy-\\nment (Raspberry Pi)'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='Flask, Git, Monitoring (Grafana, Zabbix), Edge Deploy-\\nment (Raspberry Pi)\\n• Databases: PostgreSQL, MongoDB, SQLite, MySQL\\n• Programming: Python (+7 yrs), SQL\\nSoft Skills\\n• Leadership: Taking ownership of projects, guiding technical decisions, and supporting teammates when needed.\\nExperienced in working closely with cross-functional teams.\\n• Communication: Able to explain complex ML ideas in a clear and practical way to both technical and non-technical\\naudiences.'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='audiences.\\n• Project Management: Organized and reliable when managing multiple projects, setting priorities, and meeting\\ndeadlines.\\n• Problem-Solving: Strong analytical mindset with a creative approach to finding solutions. Detail-oriented, commit-\\nted to quality, and always learning new tools or methods to improve my work.\\nSelected Certificates\\n• Coursera | 2023, Generative AI with LLMs.\\n• AWS | 2022, AWS Cloud Practitioner.\\n• Udemy | 2020, Data Science and ML in Python.'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='• AWS | 2022, AWS Cloud Practitioner.\\n• Udemy | 2020, Data Science and ML in Python.\\n• Coursera | 2021, Structuring ML Projects.\\n• Coursera\\n|\\n2019, Neural Networks and DL with\\nPython.\\nHonors & Awards\\n• Top 1%, Nationwide M.Sc. entrance exam in engineering (Konkour), with more than 25,000 participants, 2013, Iran.\\n• Top 0.5%, National entrance exam of Iran universities (Konkour), among more than 280,000 participants, 2009, Iran.\\nLanguages'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='Languages\\n• Languages: English: Fluent, French: Intermediate, Persian: Native')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    ## process text\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        ##create temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "\n",
    "        #Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "    ## process images\n",
    "    ##Three Important Actions:\n",
    "\n",
    "    ##Convert PDF image to PIL format\n",
    "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrieval\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()\n",
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4aa87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unified FAISS vector store with CLIP embeddings\n",
    "embeddings_array = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2fa6cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Create custom FAISS index since we have precomputed embeddings\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None,  # We're using precomputed embeddings\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c899cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-4 Vision model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f3c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
    "    # Embed query using CLIP\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search in unified vector store\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983f15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
    "    content = []\n",
    "    \n",
    "    # Add the query\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
    "    })\n",
    "    \n",
    "    # Separate text and image documents\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Add text context\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    \n",
    "    # Add images\n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add instruction\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217a24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrieve_multimodal(query, k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from GPT-4V\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb921b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Summarize the main findings from the document\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  - Text from page 1: Languages\n",
      "• Languages: English: Fluent, French: Intermediate, Persian: Native\n",
      "  - Text from page 1: • AWS | 2022, AWS Cloud Practitioner.\n",
      "• Udemy | 2020, Data Science and ML in Python.\n",
      "• Coursera | 20...\n",
      "  - Text from page 1: Techincal Skills\n",
      "• Generative AI & LLMs: Transformers, Multimodal\n",
      "AI, Agentic AI, LangChain, RAG, Cr...\n",
      "  - Text from page 0: collaborating with teams to turn complex ideas into practical, high-impact systems. I stay aligned w...\n",
      "  - Text from page 0: feature engineering. The approach improved forecast accuracy by 5% over the previous model.\n",
      "– Create...\n",
      "\n",
      "\n",
      "Answer: The document describes an experienced Machine Learning and AI Engineer with a strong background in developing and deploying high-impact, production-ready AI systems.\n",
      "\n",
      "Key findings include:\n",
      "*   **Extensive Technical Expertise:** Proficient in Generative AI, LLMs (Transformers, LangChain, RAG, HuggingFace, Amazon Bedrock), core ML tools (PyTorch, TensorFlow, Scikit-learn, CNN, LSTM, YOLO), and MLOps/Deployment (CI/CD, Docker, AWS, GCP, MLflow, FastAPI).\n",
      "*   **Practical Project Experience:** Developed a hybrid multimodal ML model achieving 96% accuracy for time-series forecasting, created a sky-state estimation model with over 90% accuracy, built an AI recommendation system serving 150K+ users, and developed an NLP pipeline processing 10K+ documents daily.\n",
      "*   **Certifications & Academic Excellence:** Holds an AWS Cloud Practitioner certification and completed several advanced ML/DL courses. Achieved top 1% and 0.5% in highly competitive national entrance exams in Iran for M.Sc. and university admission, respectively.\n",
      "*   **Multilingual:** Fluent in English, native in Persian, and intermediate in French.\n",
      "======================================================================\n",
      "\n",
      "Query: What visual elements are present in the document?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  - Text from page 1: Languages\n",
      "• Languages: English: Fluent, French: Intermediate, Persian: Native\n",
      "  - Text from page 1: Techincal Skills\n",
      "• Generative AI & LLMs: Transformers, Multimodal\n",
      "AI, Agentic AI, LangChain, RAG, Cr...\n",
      "  - Text from page 1: • AWS | 2022, AWS Cloud Practitioner.\n",
      "• Udemy | 2020, Data Science and ML in Python.\n",
      "• Coursera | 20...\n",
      "  - Text from page 0: collaborating with teams to turn complex ideas into practical, high-impact systems. I stay aligned w...\n",
      "  - Text from page 0: – Designed and maintained a CI/CD pipeline using GitHub Actions and AWS ECR, enabling reproducible m...\n",
      "\n",
      "\n",
      "Answer: Based on the provided text excerpts, the following visual elements related to document formatting and structure are present:\n",
      "\n",
      "*   **Headings/Section Titles:** Examples include \"Languages,\" \"Technical Skills,\" \"Honors & Awards,\" and \"Work Experiences.\" These clearly delineate different sections of the document.\n",
      "*   **Bullet Points:** The consistent use of the \"•\" symbol indicates that information is presented in bulleted lists (e.g., listing languages, technical skills, certifications, and responsibilities).\n",
      "*   **Paragraphs/Text Blocks:** The introductory statement and the detailed descriptions under work experiences are structured as distinct blocks of text.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"Summarize the main findings from the document\",\n",
    "        \"What visual elements are present in the document?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = multimodal_pdf_rag_pipeline(query)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
